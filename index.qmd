---
title: "Daily dynamics and weekly rhythms"
subtitle: "Supplemental Materials"
author: "MH Manuel Haqiqatkhah"
format:
  html:
    code-fold: true
    code-tools: true
    code-overflow: scroll
    output-file: "index.html"
    code-line-numbers: true
    toc: true
    toc-location: body
    number-sections: true
    number-offset: -1
    link-external-newwindow: true
    toc-depth: 5
    self-contained: false
    self-contained-math: false
    code-summary: "Click to expand the code"
    anchor-sections: true
    reference-location: document
    citation-location: document
    theme:
      light: litera
      dark: darkly
    # comments:
    #   hypothesis: true
  pdf:
    number-sections: true
    number-depth: 8
    #number-offset: -1
    toc: true
    output-file: Code documentations
    toc-depth: 8
## For html
execute:
  enabled: true
  eval: false
  echo: true
  include: true
  warning: false
  error: false
  freeze: auto
  cache: refresh
## For pdf
# execute:
#   enabled: true
#   echo: false
#   include: false
#   warning: false
#   error: false
#   freeze: auto
#   cache: refresh
editor: visual
# bibliography: references.bib
---

# Introduction {#sec-intro}

This document contains reproducible code of the manuscript on weekly patterns and dynamics.
Please cite as

> citation info

This document has two main sections:

In @sec-foundations (corresponding to Sections 1 and 2 of the paper), we show functions used for making the visualizations (@sec-visualization) and generating simulated data the data (@sec-simulation), and we provide the Shiny app accompanying the paper (@sec-sim-shiny).

In @sec-analysis (corresponding to Sections 3 and 4 of the paper) we provide the code for analyzing individual time series according to different models within a single function (@sec-modeling), and show the code used to run the study on the empirical data (@sec-fitting), and the code used to report the results (@sec-results).

Thanks to Dr. Aidan Wright's permission, we are sharing the data used in our study, that is, the average PA scores of 98 individuals (that had less than 50% missingness) from the dataset collected by @wright_2015_DailyInterpersonalAffective in the `data` folder.
The shared dataset does not contain demographic information, and the unique person identifies have been shuffled and stored as `id`.
Other variables in the dataset are the measurement date (`date`), the measurement time (`t`, starting from the first day of the study on 2013-02-11), the average PA time series (`y`), the weekday associated with the measurement date (`weekday`), and the weekday number (`weekday_num`) with Monday being the first day of the week.

but without the demographic information or other variables including dates empirical data used in the study, we store the individual model fits in the `fits` folder within this repository, and their aggregated summaries in the `summaries` folder.

To replicate the study from the scratch, you should first either clone the repository (using `git clone https://github.com/psyguy/WeCycle.git`) or [download the repository as a zip file](https://github.com/psyguy/WeCycle/archive/refs/heads/main.zip) and extract it on your machine.
Then you can sequentially run the `.R` files you find in the `scripts` folder.
Instead of running the scripts separately, if you have [Quarto installed](https://quarto.org/docs/get-started/), you can also compile `index.qmd` located in the root directory using Quarto .

```{r}
#| label: setup
#| echo: false
#| include: false
#| eval: true
#| cache: false
#| freeze: false

library(knitr)

scripts <- list.files(path = "scripts/",
                      full.names = TRUE)

sapply(scripts, knitr::read_chunk)
```

```{r}
#| include: false
#| echo: false
#| eval: false

<<load_packages>>
```

# Foundations {#sec-foundations}

The code used for plotting the time series and fitting the models requires the time series $y_t$ to be stored in a data.frame (which, let us call `df`) with at least three columns:

-   `t`: Indicating the time of the measurement

-   `y`: The value $y_t$ on time $t$

-   `weekday` (or `weekday_num`): The name (or number) of the weekday corresponding to `t`.
    In the case of the former, it should be in the form of capitalized three letter codes (`"Mon"`, `"Tue"`, ..., `"Sun"`).
    Note that we consider Monday to be the first day of the week, and Sunday be the 0th/7th day of the week.

The column `weekday` (and `weekday_num`) can be generated if the date (e.g., as `date`) is included in `df`, using `lubridate::wday` and setting:

```{r}
#| code-fold: show

df <- df %>% 
  mutate(weekday = lubridate::wday(date,
                                   week_start = 1,
                                   label = TRUE),
         weekday_num = lubridate::wday(date,
                                       week_start = 1,
                                       label = FALSE))
```

The missing values in the time series should be explicitly indicated with `NA` in the dataframe---that is, we should have a row for each time point---which can be achieved by:

```{r}
#| code-fold: show

df <- df %>%
  right_join(data.frame(t = min(df$t):max(df$t)),
             by = "t") %>% 
  arrange(t)
```

## Time series visualizations {#sec-visualization}

The visualizations shown in discussed in the paper were plotted using separate functions for each plot, that are named accordingly `plot_hist()` , `plot_seq()`, `plot_dowe()`, `plot_psd()`, `plot_acf()`, and `plot_pacf()`.
The main argument of these functions is `d`, which can be a numerical vector (for which the weekdays are added, starting by Monday), or it can be a dataframe with the columns specifiedexplained earlier.

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `plot_hist()`"

<<plot_hist>>
```

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `plot_seq()`"

<<plot_seq>>
```

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `plot_dowe()`"

<<plot_dowe>>
```

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `plot_psd()`"

<<plot_psd>>
```

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `plot_acf()`"

<<plot_acf>>
```

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `plot_pacf()`"

<<plot_pacf>>
```

To make sure that the data provided to these functions are in the correct format (with the above-mentioned columns), the function `data_shaper()` is called within these plotting functions to do the job.
This function also amends a new column (`week_num`) to the dataframe that counts the week number since the start of the time series, which is needed for plotting the DOWEs in `plot_dowe()`.

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `data_shaper()`"

<<data_shaper>>
```

Then, the function `plot_row_assembly()` uses the above functions to put them together in a row and returns either the plot, or saves it in a file in the `figures` folder with dimensions and sizes that render in nice proportions when the plot is saved with a `.svg` (good for putting in Word or online) or `.pdf` (good for $\LaTeX$ manuscripts) file formats.

This function takes a list of time series (either as vectors or dataframes) in its `list_data` argument, and adds vertical labels to each row of the plots with the values passed to `list_labels`.

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `plot_row_assembly()`"

<<plot_row_assembly>>
```

## Simulating time series {#sec-simulation}

Pure time series (i.e., without time index and weekday) are generated using `m_sim()` that generate $y_t = \mu_t + a_t$ by adding a deterministic vector `mu_t` to a stochastic time series `a_t`.
The innovations used in for simulation are generated from a random seed set by `seed` argument.
The arguments `burnin` indicate the number of samples thrown away as burn-in samples.

The function generates three time-varying versions of $\mu_t$ as explained in the paper in three vectors:

 - $D_t$ (as `d_t_0`) based on the weekday means (DOWEs) that are passed as a vector to `dowe` (default: all zeros);
 
 - $W_t$ (as `w_t_0`) with workdays mean of 0, and weekend effect determined by the argument `wee`; and 
 
 - $H_t$ (as `h_t_0`) with overall mean of 0 and given amplitude `amp` (default: 0), and peak shift `peak_shift` (default: 1, for Monday).

The time-varying mean `mu_t` is constructed by adding `c` to `d_t_0`, `w_t_0`, `h_t_0`.
This approach allows users to investigate the effect imposing multiple mechanisms for the mean structure $\mu_t$.
Needless to say, $D_t$, in general, can account for any arbitrary shape in the mean structure;
yet, our implementation may come in handy in the Shiny app (see @sec-sim-shiny).

The stochastic component $a_t$ is generated using `sarima::sim_sarima()`.
Since this function cannot generate white noise process, if no SAR[I]MA component is specified, `a_t` is generated manually.
In our simulation function `m_sim()` we allow including non-seasonal and seasonal differencing in the model (which we did not discuss in the paper, but implemented in the Shiny app).

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `m_sim()`"

<<m_sim>>
```

## Shiny app {#sec-sim-shiny}

To be completed later.

# Modeling time series data {#sec-modeling}

There are to functions that are used for analyzing the data:
`m_fit()` does the model fitting, and `m_estimates()` extracts the parameter estimates and post-processes them to get CIs and information criteria, and, if necessary, performs bootstrapping.

## Fitting time series models {#sec-fitting}

To carry out the analyses in the paper, we wrote a wrapper function called `m_fit()` around `forecast::Arima()` with additional capabilities which is a generalization of the code snippets shown in the paper.

This function flexibly determines the model orders by parsing the model name (as they were presented in the paper) passed as a string to `model_string`, which helps using it in elsewhere, e.g., in a Shiny app.

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `m_fit()`"

<<m_fit>>
```

This function can also save the fit object in a folder (specified by `save_folder_fit`).
To streamline the model fitting and post-processing the fit files, `m_fit()` will call `m_estimates()` if `save_est` is set to `TRUE`.

## Extracting parameter estimates {#sec-estimates}

The output of `m_fit()` is a list of class `Arima` that contains multiple components (see `?stats::arima` and `?forecast::Arima`), but does not contain the estimation standard errors and significances.
Furthermore, in case $H_t$ is included in the model, the output does not contain estimates for peak shift $\psi$ and amplitude $S$.

The function `m_estimates()` extracts the point estimates of the parameters( `m$coef`) and calculates their estimation standard errors using the estimated variance matrix of the coefficients (`m$var.coef`), and calculates 2.5% and 97.5% confidence intervals and significance of the estimates, and stores them in a dataframe called `estimates`.
The function returns a list that contains this dataframe as well as a 1-row dataframe `information_criteria` that includes different information criteria (AIC, AICc, BIC) as well as the absolute value of the log-likelihood (that could have been used as a criterion in model selection, which we did not use in our paper).

In case the harmonic mean structure is in the model, this function also calculates points estimates and their CIs of peak shift $\psi$ and amplitude $S$ by means of bootstrapping by drawing `boot_n` samples (default: 10000) from the multivariate normal distribution attributable implied by the estimated variance matrix of the coefficients.

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `m_estimates()`"

<<m_estimates>>
```

The amplitude $S$ and peak shift $\psi$ are respectively calculated by two auxiliary functions, `calc_amp()` and `calc_peak_theta()`, which are vectorized (using `Vectorize()`) to increase the performance when applied to bootstrapped values.
Note that `calc_peak_theta(a, b)` is equivalent to `base::atan2(b, a)`, which is the [2-argument arctangent function](https://en.wikipedia.org/wiki/Atan2);
we explicitly defined this function in our code to mirror the expression provided in Equation 22b of the paper.

```{r}
#| eval: true
#| code-fold: true
#| code-summary: "Click to reveal `calc_amp()` and `calc_peak_theta()`"

<<aux_functions>>
```

Note that the variable $\psi$ is circular with a period of 7, that is, peak shifts of $\psi$ and $\psi + 7$ and $\psi - 7$ are equivalent, thus its value cannot calculate summary statistics such as mean, median, and quantiles based on its $0 - 7$ values;
for instance, an individual with $\psi_1 = 1$ (peaking on Monday) is more similar to someone with $\psi_2 = 6$ (peaking on Saturday) than someone with and $\psi_3 = 4$ (peaking on Thursday).
To take the cisrularity of this variable into account, we use `circular` package [@lund_2023_CircularCircularStatistics] to transform  helps us in calculating 

This should be noted when calculating the point estimate (either by mean or median) and CIs of the bootstrapped values.



# Analyzing empirical data {#sec-analysis}

## Fitting models to the whole dataset {#sec-empirical-fitting}

```{r}
#| eval: false

d_w <- readRDS("data_private/d_w")
d_pa <- d_w %>%
  filter(item == "avg.pa") %>%
  mutate(
    weekday = lubridate::wday(date,
                              week_start = 1,
                              label = TRUE),
    weekday_num = lubridate::wday(date,
                                  week_start = 1,
                                  label = FALSE)
  ) %>%
  select(id, t, y, date, weekday, weekday_num)
```

## Results {#sec-empirical-results}
